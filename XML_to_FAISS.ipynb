{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1yB/M+NsWuE/6epQLPvkl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemhalatha/medical-rag-chatbot/blob/main/XML_to_FAISS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KN_6OmmbAnZy",
        "outputId": "fde0e463-8fc0-4605-e6f4-6090655ae72e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load XML file"
      ],
      "metadata": {
        "id": "-864pKp0A3P_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xml_path = \"/content/drive/MyDrive/mplus_topics_2025-11-20.xml\"\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "tree = ET.parse(xml_path)\n",
        "root = tree.getroot()"
      ],
      "metadata": {
        "id": "QqdxHq9tAq0C"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract text content from XML\n"
      ],
      "metadata": {
        "id": "r9Sw7awcBJag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = []\n",
        "\n",
        "def extract_text(node):\n",
        "    if node.text and node.text.strip():\n",
        "        texts.append(node.text.strip())\n",
        "    for child in node:\n",
        "        extract_text(child)\n",
        "\n",
        "extract_text(root)\n"
      ],
      "metadata": {
        "id": "AxYw9pgPAx8_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean & filter text"
      ],
      "metadata": {
        "id": "epD1hpU5BNlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned = []\n",
        "for t in texts:\n",
        "    t = t.replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()\n",
        "    if len(t) > 20:     # keep meaningful sentences\n",
        "        cleaned.append(t)"
      ],
      "metadata": {
        "id": "TG4yDiQXBS5H"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Chunk the text"
      ],
      "metadata": {
        "id": "9Oemgy8lBcde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = []\n",
        "chunk_size = 300\n",
        "chunk_id = 0\n",
        "\n",
        "for text in cleaned:\n",
        "    words = text.split()\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "        chunk = \" \".join(words[i:i+chunk_size])\n",
        "        chunks.append({\"id\": f\"chunk_{chunk_id}\", \"text\": chunk})\n",
        "        chunk_id += 1"
      ],
      "metadata": {
        "id": "cYW8NNIjBdgO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build FAISS DB using SentenceTransformer embeddings\n"
      ],
      "metadata": {
        "id": "lkvS5UjOBko6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q faiss-cpu sentence-transformers"
      ],
      "metadata": {
        "id": "1A9r3esABwd0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "corpus = [c[\"text\"] for c in chunks]\n",
        "embeddings = model.encode(corpus, convert_to_numpy=True)\n",
        "\n",
        "d = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(embeddings)"
      ],
      "metadata": {
        "id": "7HnrldU0BmU-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save FAISS DB + Metadata"
      ],
      "metadata": {
        "id": "C0uDd7khB41G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faiss.write_index(index, \"/content/drive/MyDrive/medline_faiss.index\")\n",
        "\n",
        "import json\n",
        "with open(\"/content/drive/MyDrive/medline_chunks.json\", \"w\") as f:\n",
        "    json.dump(chunks, f, indent=2)\n",
        "\n",
        "print(\"FAISS index and metadata saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXxpAtdBB6Wh",
        "outputId": "eb547451-47cd-4139-aa12-478c05f0d545"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index and metadata saved.\n"
          ]
        }
      ]
    }
  ]
}